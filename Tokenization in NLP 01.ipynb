{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd320878",
   "metadata": {},
   "source": [
    "# <center>NLPðŸ’¬ðŸ”‰ By ðŸŽ¯Udaya ( Data Engineer ðŸ“š) </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97250e7f",
   "metadata": {},
   "source": [
    "## Corpus\n",
    "A large collection of documents used for training NLP models. The corpus provides the data from which the vocabulary is built and upon which the models are trained.\n",
    "#### Example: [\"I love NLP.\", \"NLP is fun.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "930ff917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus:ðŸ‘‰  ['I love NLP.', 'NLP is fun.']\n"
     ]
    }
   ],
   "source": [
    "corpus = [\"I love NLP.\", \"NLP is fun.\"]\n",
    "print(\"Corpus:ðŸ‘‰ \", corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96b31a9",
   "metadata": {},
   "source": [
    "## Document\n",
    "Definition: A single piece of text, such as an article, a paragraph, or a sentence. It is usually the unit of analysis in text processing.\n",
    "#### Example: \"I love NLP.\" is one document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d5795d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document:ðŸ‘‰  I love NLP.\n"
     ]
    }
   ],
   "source": [
    "document = \"I love NLP.\"\n",
    "print(\"Document:ðŸ‘‰ \", document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52288a96",
   "metadata": {},
   "source": [
    "## Words\n",
    "Definition: The basic units of language, representing distinct meanings. Words can be in different forms, such as singular, plural, verbs, nouns, etc.\n",
    "#### Example: \"cat\", \"running\", \"beautiful\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bda30d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words:ðŸ‘‰  ['cat', 'running', 'beautiful']\n"
     ]
    }
   ],
   "source": [
    "words = [\"cat\", \"running\", \"beautiful\"]\n",
    "print(\"Words:ðŸ‘‰ \", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b994025",
   "metadata": {},
   "source": [
    "## Tokens\n",
    "Definition: Tokens are individual pieces of a sentence or text that have been segmented, often corresponding to words, punctuation, or other meaningful elements.\n",
    "#### Example: In the sentence \"I love NLP.\", the tokens are [\"I\", \"love\", \"NLP\", \".\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9588045c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:ðŸ‘‰  ['I', 'love', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"I love NLP.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "print(\"Tokens:ðŸ‘‰ \", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51f90b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bc4328",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "Definition: The set of unique tokens found in a corpus. This represents the words and symbols that the model or algorithm has been trained to recognize and understand.\n",
    "#### Example: If our corpus is [\"I love NLP\", \"NLP is fun\"], the vocabulary is {\"I\", \"love\", \"NLP\", \"is\", \"fun\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5828d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fd9b21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:ðŸ‘‰  {'is', 'fun', 'love', 'NLP', 'I'}\n"
     ]
    }
   ],
   "source": [
    "documents = [\"I love NLP\", \"NLP is fun\"]\n",
    "\n",
    "tokenized_documents = [word_tokenize(doc) for doc in documents]\n",
    "vocabulary = set(token for doc in tokenized_documents for token in doc)\n",
    "print(\"Vocabulary:ðŸ‘‰ \", vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d72bfc",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Definition: The process of breaking down a text into smaller units, typically tokens. Tokenization is a crucial preprocessing step in NLP as it transforms raw text into a format that can be analyzed and processed by algorithms.\n",
    "#### Example: Tokenizing the sentence \"Tokenization is fun!\" results in [\"Tokenization\", \"is\", \"fun\", \"!\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4cade7",
   "metadata": {},
   "source": [
    "[NLTK - Natural Language Toolkit](https://www.nltk.org/)\n",
    "\n",
    "[spaCy](https://spacy.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d32f50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef915f09",
   "metadata": {},
   "source": [
    "## Types of Tokenization\n",
    "### Word Tokenization:\n",
    "\n",
    "Breaks text into individual words or tokens based on whitespace or punctuation.\n",
    "#### Example: \"I love NLP.\" -> [\"I\", \"love\", \"NLP\", \".\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd9c89c",
   "metadata": {},
   "source": [
    "`word_tokenize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf84cd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization:ðŸ‘‰  ['I', 'love', 'NLP', '.', 'NLP', 'is', 'fun', '!']\n"
     ]
    }
   ],
   "source": [
    "text = \"I love NLP. NLP is fun!\"\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokens = word_tokenize(text)\n",
    "\n",
    "print(\"Word Tokenization:ðŸ‘‰ \", word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e02f30",
   "metadata": {},
   "source": [
    "`wordpunct_tokenize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79426eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome',\n",
       " 'to',\n",
       " ',',\n",
       " 'my',\n",
       " 'mastery',\n",
       " 'map',\n",
       " 'of',\n",
       " '#',\n",
       " 'NLP',\n",
       " '.',\n",
       " 'Do',\n",
       " 'follow',\n",
       " 'me',\n",
       " '!']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "corpus = \"\"\" Welcome to, my mastery map of #NLP. \n",
    "Do follow me ! \"\"\"\n",
    "\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77779f3e",
   "metadata": {},
   "source": [
    "`word_tokenize`  v/s `wordpunct_tokenize`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f7f950",
   "metadata": {},
   "source": [
    "### wordpunct_tokenize\n",
    "Function: Splits text into words and separates all punctuation characters.\n",
    "#### Example:\n",
    "* Input: \"Hello, world!\"\n",
    "* Output: ['Hello', ',', 'world', '!']\n",
    "* Use Case: When you need each punctuation mark to be a separate token.\n",
    "### word_tokenize\n",
    "Function: Splits text into words while treating punctuation more contextually, often keeping contractions and other linguistic elements together.\n",
    "#### Example:\n",
    "* Input: \"Don't go.\"\n",
    "* Output: ['Do', \"n't\", 'go', '.']\n",
    "* Use Case: When you need a more nuanced handling of punctuation and contractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82eea5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‡ðŸ‘‡ðŸ‘‡ðŸ‘‡ðŸ‘‡\n",
      "wordpunct_tokenize: ['Don', \"'\", 't', 'go', '.', 'Hello', ',', 'world', '!']\n",
      "ðŸ‘‡ðŸ‘‡ðŸ‘‡ðŸ‘‡ðŸ‘‡\n",
      "word_tokenize: ['Do', \"n't\", 'go', '.', 'Hello', ',', 'world', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "text = \"Don't go. Hello, world!\"\n",
    "\n",
    "print('ðŸ‘‡ðŸ‘‡ðŸ‘‡ðŸ‘‡ðŸ‘‡')\n",
    "# Using wordpunct_tokenize\n",
    "tokens_wp = wordpunct_tokenize(text)\n",
    "print(\"wordpunct_tokenize:\", tokens_wp)\n",
    "\n",
    "print('ðŸ‘‡ðŸ‘‡ðŸ‘‡ðŸ‘‡ðŸ‘‡')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Don't go. Hello, world!\"\n",
    "\n",
    "# Using word_tokenize\n",
    "tokens_wt = word_tokenize(text)\n",
    "print(\"word_tokenize:\", tokens_wt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561e5ae4",
   "metadata": {},
   "source": [
    "### Sentence Tokenization:\n",
    "\n",
    "Splits text into individual sentences based on punctuation marks like periods, exclamation points, or question marks.\n",
    "#### Example: \"I love NLP. NLP is fun!\" -> [\"I love NLP.\", \"NLP is fun!\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecf956cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization:ðŸ‘‰  ['I love NLP.', 'NLP is fun!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"I love NLP. NLP is fun!\"\n",
    "sentence_tokens = sent_tokenize(text)\n",
    "\n",
    "print(\"Sentence Tokenization:ðŸ‘‰ \", sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f28ed9f",
   "metadata": {},
   "source": [
    "### Whitespace Tokenization:\n",
    "\n",
    "Splits text based on whitespace characters like spaces, tabs, or newlines.\n",
    "#### Example: \"I love NLP.\" -> [\"I\", \"love\", \"NLP.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ddeb69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace Tokenization:ðŸ‘‰  ['I', 'love', 'NLP.', 'NLP', 'is', 'fun!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "text = \"I love NLP. NLP is fun!\"\n",
    "\n",
    "whitespace_tokenizer = WhitespaceTokenizer()\n",
    "whitespace_tokens = whitespace_tokenizer.tokenize(text)\n",
    "\n",
    "print(\"Whitespace Tokenization:ðŸ‘‰ \", whitespace_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580445b1",
   "metadata": {},
   "source": [
    "### Regular Expression Tokenization:\n",
    "\n",
    "Tokenizes text based on specified patterns using regular expressions.\n",
    "#### Example: Tokenizing based on all alphabetical characters -> \"I love NLP.\" -> [\"I\", \"love\", \"NLP\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d358366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular Expression Tokenization:ðŸ‘‰  ['I', 'love', 'NLP', 'NLP', 'is', 'fun']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "text = \"I love NLP. NLP is fun!\"\n",
    "\n",
    "pattern = r'\\w+'\n",
    "regexp_tokenizer = RegexpTokenizer(pattern)\n",
    "regexp_tokens = regexp_tokenizer.tokenize(text)\n",
    "\n",
    "print(\"Regular Expression Tokenization:ðŸ‘‰ \", regexp_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15db15ac",
   "metadata": {},
   "source": [
    "### NGram Tokenization:\n",
    "\n",
    "Creates tokens by combining N consecutive words from the text.\n",
    "#### Example: Bigram tokenization -> \"I love NLP.\" -> [\"I love\", \"love NLP\", \"NLP .\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78ebefa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‰  ['I', 'love', 'NLP', '.', 'NLP', 'is', 'fun', '!']\n",
      "N-Gram Tokenization (Bigrams):ðŸ‘‰  [('I', 'love'), ('love', 'NLP'), ('NLP', '.'), ('.', 'NLP'), ('NLP', 'is'), ('is', 'fun'), ('fun', '!')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "text = \"I love NLP. NLP is fun!\"\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokens = word_tokenize(text)\n",
    "print(\"ðŸ‘‰ \",word_tokens)\n",
    "\n",
    "n = 2\n",
    "ngram_tokens = list(ngrams(word_tokens, n))\n",
    "print(\"N-Gram Tokenization (Bigrams):ðŸ‘‰ \", ngram_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be9706a",
   "metadata": {},
   "source": [
    "### Custom Tokenization:\n",
    "\n",
    "Tailors tokenization rules based on specific requirements or domain-specific knowledge.\n",
    "#### Example: Tokenizing text in a medical domain may involve recognizing medical terms or abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5582a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Tokenization (Medical Terms and Abbreviations):ðŸ‘‰  ['MRI', 'brain', 'COPD', 'lung']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def custom_tokenize(text):\n",
    "    medical_terms_pattern = r'(?:\\b(?:heart|lung|brain)\\b)|(?:\\b(?:COPD|MRI|ECG)\\b)'\n",
    "    tokens = re.findall(medical_terms_pattern, text, flags=re.IGNORECASE)\n",
    "    return tokens\n",
    "medical_text = \"The patient underwent an MRI scan to examine the brain. COPD is a chronic lung disease.\"\n",
    "custom_tokens = custom_tokenize(medical_text)\n",
    "print(\"Custom Tokenization (Medical Terms and Abbreviations):ðŸ‘‰ \", custom_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fca1d93",
   "metadata": {},
   "source": [
    "#### More on Tokenization ðŸ‘‡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a0c5b9",
   "metadata": {},
   "source": [
    "### Treebank Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b87d7aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome',\n",
       " 'to',\n",
       " ',',\n",
       " 'my',\n",
       " 'mastery',\n",
       " 'map',\n",
       " 'of',\n",
       " '#',\n",
       " 'NLP.',\n",
       " 'Do',\n",
       " 'follow',\n",
       " 'me',\n",
       " '!']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "TBWT = TreebankWordTokenizer()\n",
    "\n",
    "corpus = \"\"\" Welcome to, my mastery map of #NLP. \n",
    "Do follow me ! \"\"\"\n",
    "\n",
    "TBWT.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69c0a28",
   "metadata": {},
   "source": [
    "`TreebankWordTokenizer` v/s `wordpunct_tokenize`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e076152c",
   "metadata": {},
   "source": [
    "### TreebankWordTokenizer\n",
    "Function: Tokenizes text using the conventions of the Penn Treebank.\n",
    "#### Example:\n",
    "* Input: \"Dr. Smith's cat doesn't like fish.\"\n",
    "* Output: ['Dr.', 'Smith', \"'s\", 'cat', 'does', \"n't\", 'like', 'fish', '.']\n",
    "* Special Handling: Keeps some punctuation (e.g., periods in abbreviations) together with words and splits contractions properly.\n",
    "### wordpunct_tokenize\n",
    "Function: Splits text into words and separates all punctuation.\n",
    "#### Example:\n",
    "* Input: \"Dr. Smith's cat doesn't like fish.\"\n",
    "* Output: ['Dr', '.', 'Smith', \"'\", 's', 'cat', 'doesn', \"'\", 't', 'like', 'fish', '.']\n",
    "* Special Handling: Treats every punctuation mark as a separate token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9687d4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordpunct_tokenize: ðŸ‘‰ ['Follow', '!', 'Udaya', 'on', 'ðŸ‘‰', 'https', '://', 'github', '.', 'com', '/', 'codeWudaya']\n",
      "ðŸ‘‡ðŸ‘‡ðŸ‘‡ðŸ‘‡ðŸ‘‡ðŸ‘‡ðŸ‘‡\n",
      "TreebankWordTokenizer:ðŸ‘‰  ['Follow', '!', 'Udaya', 'on', 'ðŸ‘‰', 'https', ':', '//github.com/codeWudaya']\n"
     ]
    }
   ],
   "source": [
    "# Using wordpunct_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "text = \"Follow! Udaya on ðŸ‘‰ https://github.com/codeWudaya \"\n",
    "tokens_wp = wordpunct_tokenize(text)\n",
    "print(\"wordpunct_tokenize: ðŸ‘‰\", tokens_wp)\n",
    "\n",
    "print(\"ðŸ‘‡ðŸ‘‡ðŸ‘‡ðŸ‘‡ðŸ‘‡ðŸ‘‡ðŸ‘‡\")\n",
    "\n",
    "# Using TreebankWordTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "text = \"Follow! Udaya on ðŸ‘‰ https://github.com/codeWudaya \"\n",
    "\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "tokens_tb = treebank_tokenizer.tokenize(text)\n",
    "print(\"TreebankWordTokenizer:ðŸ‘‰ \", tokens_tb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
